version: '3.8'

services:
  yolo-detector:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bengawan-object-detection
    
    # Enable GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Mount volumes for persistent data
    volumes:
      - ./datasets:/app/datasets
      - ./runs:/app/runs
      - ./model_conversion:/app/model_conversion
      - ./configs:/app/configs
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # Keep container running for interactive use
    stdin_open: true
    tty: true
    
    # Working directory
    working_dir: /app
    
    # Default command
    command: python train.py

  # Service for inference only (lighter container)
  yolo-inference:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bengawan-inference
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    volumes:
      - ./runs:/app/runs
      - ./inference:/app/inference
      - ./model_conversion:/app/model_conversion
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    
    stdin_open: true
    tty: true
    working_dir: /app
    
    profiles:
      - inference
