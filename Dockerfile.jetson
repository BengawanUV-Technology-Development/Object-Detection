# =================================================
# Dockerfile for Jetson Nano (JetPack 4.x)
# =================================================
# Base image: NVIDIA L4T with PyTorch pre-installed
# Supports: Python 3.6, TensorRT, CUDA 10.2
# =================================================

# Use NVIDIA L4T PyTorch base image for Jetson
# JetPack 4.6.1 (L4T R32.7.1) with PyTorch 1.10
FROM nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.10-py3

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

# L4T image already has most dependencies pre-installed
# Only install minimal additional packages via pip
# OpenCV is already included in L4T PyTorch image

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir \
    "numpy>=1.18.5,<1.20" \
    "Pillow==8.4.0" \
    "PyYAML>=5.3.1" \
    "tqdm>=4.64.0"

# Install ONNX Runtime for Jetson (pre-built wheel from NVIDIA)
# First install dependencies with Python 3.6 compatible versions
RUN pip3 install --no-cache-dir "protobuf==3.19.6" "flatbuffers==2.0"

# Download and install ONNX Runtime GPU for Jetson
# Source: https://elinux.org/Jetson_Zoo#ONNX_Runtime
RUN wget -q https://nvidia.box.com/shared/static/pmsqsiaw4pg9qrbeckcbymho6c01jj4z.whl -O onnxruntime_gpu-1.11.0-cp36-cp36m-linux_aarch64.whl && \
    pip3 install --no-deps onnxruntime_gpu-1.11.0-cp36-cp36m-linux_aarch64.whl && \
    rm onnxruntime_gpu-1.11.0-cp36-cp36m-linux_aarch64.whl

# Copy inference scripts
COPY inference/ ./inference/

# Create directories for models
RUN mkdir -p /app/models /app/outputs

# Expose port for streaming (optional)
EXPOSE 8080

# Default command - run inference
CMD ["python3", "inference/jetson_inference.py", "--model", "models/best.onnx", "--source", "0"]
