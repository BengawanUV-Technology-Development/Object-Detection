# =================================================
# Dockerfile for Jetson Nano (JetPack 4.x)
# =================================================
# Base image: NVIDIA L4T with PyTorch pre-installed
# Supports: Python 3.6, TensorRT, CUDA 10.2
# =================================================

# Use NVIDIA L4T PyTorch base image for Jetson
# JetPack 4.6.1 (L4T R32.7.1) with PyTorch 1.10
FROM nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.10-py3

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

# L4T image already has most dependencies pre-installed
# Only install minimal additional packages via pip
# OpenCV is already included in L4T PyTorch image

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir \
    numpy>=1.18.5 \
    Pillow>=7.1.2 \
    PyYAML>=5.3.1 \
    tqdm>=4.64.0

# Install ONNX Runtime for inference (use CPU version, more compatible)
RUN pip3 install --no-cache-dir onnxruntime

# Copy inference scripts
COPY inference/ ./inference/

# Create directories for models
RUN mkdir -p /app/models /app/outputs

# Expose port for streaming (optional)
EXPOSE 8080

# Default command - run inference
CMD ["python3", "inference/jetson_inference.py", "--model", "models/best.onnx", "--source", "0"]
